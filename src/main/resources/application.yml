# Kafka-S3-Delta Connector - Simplified Kafka-Native Idempotency
# Streams data from Kafka topics to S3 as Delta Lake tables using Kafka's built-in offset management
spring:
  application:
    name: kafka-s3-delta-connector-simple
  profiles:
    active: local
      
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: kafka-s3-delta-connector
      auto-offset-reset: earliest
      enable-auto-commit: false
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      max-poll-interval-ms: 300000
      session-timeout-ms: 30000
      fetch-max-wait-ms: 500
      
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

# Schema Registry Configuration
schema:
  registry:
    url: http://localhost:8081

# S3-Compatible Storage Configuration (MinIO for local development)
aws:
  s3:
    endpoint: http://localhost:9000
    region: us-east-1
    path-style-access: true
    access-key-id: minioadmin
    secret-access-key: minioadmin

# Delta Lake Connector Configuration
connector:
  topics:
    # User Events -> COB-partitioned Delta Lake Table (supports JSON/CSV/Avro)
    user-events:
      kafka-topic: "user.events.v1"
      avro-schema: "UserEvent"
      supported-formats: ["JSON", "CSV", "AVRO"]
      destination:
        bucket: "data-lake"
        path: "events/user-events"
        partition-columns: ["cob_date"]  # COB date partitioning
        table-name: "user_events"
        partition-strategy: "cob-based"
        delta-config:
          enable-optimize: true
          optimize-interval: 10
          enable-schema-evolution: true
          enable-vacuum: false
          checkpoint-interval: "10"
      processing:
        batch-size: 1000
        flush-interval: 60
        max-retries: 3
    
    # Order Events -> Delta Lake Table  
    order-events:
      kafka-topic: "orders.lifecycle.v2"
      schema-file: "schemas/order-events-schema.json"
      destination:
        bucket: "data-lake"
        path: "orders/lifecycle-events"
        partition-columns: ["year", "month", "day", "order_status"]  # Status-based partitioning
        table-name: "order_events"
        delta-config:
          enable-optimize: true
          optimize-interval: 5
          enable-vacuum: true
          enable-schema-evolution: true
          vacuum-retention-hours: 168  # 7 days
      processing:
        batch-size: 500
        flush-interval: 30
        max-retries: 5

# Kafka-Native Idempotency Configuration (minimal settings)
kafka-native:
  idempotency:
    # Relying on Kafka's offset management - no additional configuration needed
    # Consumer groups ensure only one instance per partition
    # Manual acknowledgment ensures offsets committed only after successful processing
    enabled: true

# COB Processing Configuration  
cob:
  partitioning:
    default-column: "cob_date"
    validation:
      strict-format: true
      allow-future-dates: false
      max-days-in-past: 365

# Logging Configuration
logging:
  level:
    root: INFO
    com.company.kafkaconnector: DEBUG
    org.apache.kafka: WARN
    org.apache.spark: WARN
    io.delta: INFO
  pattern:
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level [%X{correlationId}] %logger{36} - %msg%n"
    console: "%d{HH:mm:ss.SSS} [%thread] %-5level [%X{correlationId}] %logger{20} - %msg%n"
  file:
    name: logs/kafka-s3-connector.log

# Server Configuration
server:
  port: 8080
  servlet:
    context-path: /

# Management/Actuator Configuration
management:
  endpoints:
    web:
      exposure:
        include: health,metrics,info,prometheus,env,configprops
      base-path: /actuator
  endpoint:
    health:
      show-details: when-authorized
      show-components: always
    metrics:
      enabled: true
    prometheus:
      enabled: true
  server:
    port: 8081
  metrics:
    export:
      prometheus:
        enabled: true
        step: PT1M
    distribution:
      percentiles-histogram:
        http.server.requests: true
        kafka.connector: true
      percentiles:
        http.server.requests: 0.50,0.90,0.95,0.99
        kafka.connector: 0.50,0.90,0.95,0.99
  health:
    defaults:
      enabled: true